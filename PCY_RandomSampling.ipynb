{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##hash function"
      ],
      "metadata": {
        "id": "PzcfK9Gqty9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "start = time.time()\n",
        "\n",
        "#hash function\n",
        "def hash(element1,element2,num_trans):\n",
        "  result = (element1 + element2) % num_trans\n",
        "  return result"
      ],
      "metadata": {
        "id": "3FBVuzYAtyPl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data && generate C1 and pairs for bucket && get random sample of the data with size(1/4)"
      ],
      "metadata": {
        "id": "D2cLwkQlz54F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6AM2UKXzJis"
      },
      "outputs": [],
      "source": [
        "candidates = {}\n",
        "buckets = {}\n",
        "num_trans = 0\n",
        "all_data = []\n",
        "all_data_sample = []\n",
        "#read all basket from file\n",
        "with open('retail.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    num_trans += 1\n",
        "    line_list = [int(x) for x in line.strip().split()]\n",
        "    all_data.append(line_list)\n",
        "print(len(all_data))\n",
        "\n",
        "#get random portion of 25% of the file \n",
        "all_data_sample = random.sample(all_data, int(num_trans/4))\n",
        "print(len(all_data_sample))\n",
        "\n",
        "#count item and hash pairs for sample dataset\n",
        "def read_data(all_data_sample,num_trans,candidates,buckets):\n",
        "  for line_list in all_data_sample:  \n",
        "      for item in line_list:\n",
        "        if item in candidates.keys():\n",
        "          candidates.update({item:(candidates[item]+1)})\n",
        "        else:\n",
        "          candidates.update({item:1})\n",
        "      pairs = list(itertools.combinations(line_list, 2))\n",
        "      for pair in pairs:\n",
        "        index = hash(pair[0], pair[1],int(num_trans/4))\n",
        "        if index in buckets.keys():\n",
        "          buckets.update({index:(buckets[index]+1)})\n",
        "        else:\n",
        "          buckets.update({index:1})\n",
        "  return candidates,buckets\n",
        "#netflix.data retail.txt\n",
        "\n",
        "# get c1,buckets from sample dataset\n",
        "c1,buckets = read_data(all_data_sample,num_trans,candidates,buckets)\n",
        "sample_size = len(all_data_sample)\n",
        "min_support = 0.01\n",
        "#sample size threshold\n",
        "threshold = int(num_trans*min_support*(1/4))\n",
        "\n",
        "#original dataset size threshold\n",
        "true_threshold = int(num_trans*min_support)\n",
        "#0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Bitmap\n"
      ],
      "metadata": {
        "id": "KSurVJ3d_UYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate bit vector based on the sample size\n",
        "def generate_Bitmap(hashtable,sample_size,threshold):\n",
        "  bitmap = [0]*sample_size\n",
        "  for bucket,bucket_count in hashtable.items():\n",
        "    if bucket_count >= threshold:\n",
        "      bitmap[bucket] = 1\n",
        "  return bitmap"
      ],
      "metadata": {
        "id": "MeaO6owz_agJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate L1"
      ],
      "metadata": {
        "id": "SKCQRNUEvKCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_L(candidates,threshold):\n",
        "  L = []\n",
        "  for key,value in candidates.items():\n",
        "    if value >= threshold:\n",
        "      L.append(key)\n",
        "  return L\n",
        "\n",
        "#get frequent items based on the sample dataset\n",
        "L1 = get_L(c1,threshold)"
      ],
      "metadata": {
        "id": "Vo6a60n-vXSy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Count initially_pruned C2"
      ],
      "metadata": {
        "id": "wH3KVItd22RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_C2(candidates,dataset):\n",
        "  for basket in dataset:\n",
        "    for key in candidates.keys():\n",
        "      if set(key).issubset(set(basket)):\n",
        "        candidates.update({key:candidates[key]+1})\n",
        "  return candidates"
      ],
      "metadata": {
        "id": "hC4ypDox24Tr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate C2 && bitmap"
      ],
      "metadata": {
        "id": "IRA-gsd5xwFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate C2 based on the frequent items from sample dataset\n",
        "C2 = list(itertools.combinations(L1, 2))\n",
        "\n",
        "bitmap = generate_Bitmap(buckets,sample_size,threshold)\n",
        "\n",
        "print(len(bitmap))"
      ],
      "metadata": {
        "id": "bMwjCd0Wxygu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate L2 based on the sample dataset and verified L2 based on the original dataset"
      ],
      "metadata": {
        "id": "aPdfeXSjzJJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_L2(C2,bitmap,all_data_sample,all_data,threshold,true_threshold):\n",
        "  for pair in C2:\n",
        "    index = hash(pair[0], pair[1],int(num_trans/4))\n",
        "    if bitmap[index] != 1:\n",
        "      C2.remove(pair)\n",
        "  if len(C2) == 0:\n",
        "    return None\n",
        "  else:\n",
        "    #get L2 from sample dataset\n",
        "    initial_value = [0]*len(C2)\n",
        "    C2_initial = dict(zip(C2, initial_value))\n",
        "    C2_initial = get_C2(C2_initial,all_data_sample)\n",
        "    L2_sample = get_L(C2_initial,threshold)\n",
        "    #get verified L2 through sample L2 and original dataset\n",
        "    sample_value = [0]*len(L2_sample)\n",
        "    L2_sample_initial = dict(zip(L2_sample, sample_value))\n",
        "    L2_sample_count = get_C2(L2_sample_initial,all_data)\n",
        "    L2 = get_L(L2_sample_count,true_threshold)\n",
        "    return L2_sample,L2\n",
        "\n",
        "L2_sample,L2 = get_L2(C2,bitmap,all_data_sample,all_data,threshold,true_threshold)\n",
        "\n",
        "print(\"pairs containing false positive: \",L2_sample,\"\\nthe length of it: \" , len(L2_sample))\n",
        "\n",
        "print(\"pairs after verifying: \",L2,\"\\nthe length of it: \" , len(L2))\n",
        "\n",
        "\n",
        "print(\"--- seconds---\" , (time.time() - start))"
      ],
      "metadata": {
        "id": "-b9qXnPIzI-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}